import typer
import torch
import json
import os
import dagshub
import torch_geometric as tg
import pytorch_lightning as pl
from pydantic import BaseModel
# from dgd.datasets import fo2_dataset
from datasets import fo2_dataset
from mlflow_utils import SafeMLFlowLogger
import re
from typing import Optional
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.strategies import DDPStrategy
import random
import time
# 过滤torchdata的datapipes弃用警告

app = typer.Typer(pretty_exceptions_enable=False)


def convert_wfomi_format_to_shiny(path: str) -> str:
    if path == None:
        return None
    parent_dir = os.path.dirname(path)
    # 获取输入文件的文件名（如 "0.json"）
    filename = os.path.basename(path)

    # 生成对应的txt文件名（替换.json为.txt）
    if filename.endswith(".json"):
        txt_filename = filename.replace(".json", ".txt", 1)  # 只替换最后一个.json
    else:
        # 若输入文件不是.json，按原逻辑生成新路径（保持兼容性）
        txt_filename = f"{filename}.txt"

    # 拼接目标txt文件的完整路径
    target_txt_path = os.path.join(parent_dir, txt_filename)

    # 2. 检查目标txt文件是否已存在
    if os.path.exists(target_txt_path) and os.path.isfile(target_txt_path):
        # 若存在，直接返回该路径
        return target_txt_path
    with open(path) as f:
        data = sorted(json.load(f))

    new_path = path.replace(".json", ".txt")

    with open(new_path, "w") as f:
        for idx, sample_as_list in enumerate(data):
            sample_as_list = sorted(sample_as_list)
            f.write(json.dumps(sample_as_list) + "\n")

    return new_path


@app.command()
def main(  # classify_two_mlns
        folder: str,
        val_folder: Optional[str] = None,
        train_size: int = 0,
        val_size: int = 0,
        seed: int = 0,
        gpu: int = 0,
        dry: bool = False,
        user: str = "train500_val10k_v10_sum",
):
    data_family = folder.split("json/")[1].split("/")[0].split("_")[0][:-1]
    print(folder.split("domain")[1].split("/"))
    domain_size = int(folder.split("domain")[1].split("/")[0])
    path_1 = convert_wfomi_format_to_shiny(f"{folder}/0.json")
    path_2 = convert_wfomi_format_to_shiny(f"{folder}/1.json")
    run_name = "-".join(folder.split("/")[2:])

    print(
        f"Training on {path_1=} vs. {path_2=}, {data_family}, {domain_size}, {run_name=}"
    )
    val_txt_files = None
    if val_folder != None:
        val_path = val_folder + '/' + folder.split('/')[-1]
        val_txt_path1 = convert_wfomi_format_to_shiny(f"{val_path}/0.json")
        val_txt_path2 = convert_wfomi_format_to_shiny(f"{val_path}/1.json")
        val_txt_files = [val_txt_path1, val_txt_path2]

    train(
        config_name="vs",
        train_path1=path_1,
        domain_size=domain_size,
        data_family=data_family,
        val_paths=val_txt_files,
        train_path2=path_2,
        gpu=gpu,
        dry=dry,
        user=user,
        run_name=run_name,
        auto_train_ratio=0.7,
        seed=seed,
        train_size=train_size,
        val_size=val_size
    )



def train(
        config_name: str,
        val_paths: Optional[str],
        train_path1: Optional[str],
        domain_size: int,
        data_family: str,
        train_path2: str,
        gpu: int,
        dry: bool,
        user: str,
        run_name: str,
        auto_train_ratio: float,
        seed: int,
        train_size: int,
        val_size: int
) -> None:
    cfg = CFG(
        train=TrainConfig(batch_size=128, num_workers=8, n_train_data=-1),
        general=GeneralConfig(name=config_name),
    )
    # pl.seed_everything(seed)
    # 随机种子
    random_seed = random.randint(0, 10000)
    pl.seed_everything(random_seed)
    datamodule = fo2_dataset.FO2DataModule(
        train_size=train_size,
        val_size=val_size,
        val_paths=val_paths,
        train_path1=train_path1,
        train_path2=train_path2,
        cfg=cfg,
        auto_train_ratio=auto_train_ratio,
    )
    datamodule.prepare_data()
    n_node_classes = len(datamodule.types)
    # Data generated by digress have one more "dummy" class.
    data_family_name_to_n_node_classes = {
        "deskmate-students": 1,
        "exists-friends-person": 3,
        "friends-person": 3,
        "random-person": 2,
        "weightedcolors-v": 5,
        "color": 3,
        "colorskip": 4
    }
    data_family_name_to_n_edge_classes = {
        "deskmate-students": 3,
    }
    dir_path = os.path.dirname(train_path2)
    file_name = "train_size"+str(train_size)+"_val_size"+str(val_size)+".json"
    full_file_path = os.path.join(dir_path, file_name)
    gnn = GNN(
        n_node_classes=n_node_classes,
        n_edge_classes=data_family_name_to_n_edge_classes.get(data_family, 1),
        domain_size=domain_size,
        out_channels=256,
        gnn_layer="DenseSAGEConv",
        # gnn_layer='SAGEConv',
        # gnn_layer="GATConv",
        depth=4,
        val_metrics_file=full_file_path,
    )

    if os.environ.get("DAGSHUB_REPO_OWNER") or os.environ.get("DAGSHUB_REPO_NAME"):
        dagshub.init(
            repo_owner=os.environ.get("DAGSHUB_REPO_OWNER"),
            repo_name=os.environ.get("DAGSHUB_REPO_NAME"),
            mlflow=True,
        )

    logger_mlflow = (
        SafeMLFlowLogger(
            run_name=run_name,
            experiment_name=f"SHINY - {user}",
            tags={
                "mlflow.note.content": f"""
{val_paths=}
{train_path1=}
{train_path2=}
{auto_train_ratio=}
""",
            },
            tracking_uri=os.environ.get("MLFLOW_TRACKING_URI", "http://localhost:5000"),
        )
        if not dry
        else None
    )
    if logger_mlflow is not None:
        logger_mlflow.log_hyperparams(
            {
                "config_name": config_name,
                "data_name": val_paths,
                "train_path": train_path1,
                "domain_size": domain_size,
                "batch_size": cfg.train.batch_size,
                "data_family": data_family,
                "generated_path": train_path2,
                "run_name": run_name,
                "auto_train_ratio": auto_train_ratio,
                "seed": seed,
                "model": "GNN_sum",
                "EarlyStopping": 10,
                "auto_train_ratio": 1,
            }
        )

    trainer = pl.Trainer(
        max_epochs=1000,
        accelerator="cuda" if 0 <= gpu else "cpu",
        devices=[0] if 0 <= gpu else "auto",
        # strategy=DDPStrategy(find_unused_parameters=True),
        check_val_every_n_epoch=1,
        logger=[logger_mlflow] if logger_mlflow is not None else None,
        callbacks=[EarlyStopping(monitor="val_accuracy", mode="max", patience=10)],
    )
    trainer.fit(
        model=gnn,
        train_dataloaders=datamodule.dataloaders["train_and_gen"],
        val_dataloaders=datamodule.dataloaders["test_and_gen"],
    )



class TrainConfig(BaseModel):
    batch_size: int
    num_workers: int
    n_train_data: int


class GeneralConfig(BaseModel):
    name: str


class CFG(BaseModel):
    train: TrainConfig
    general: GeneralConfig


class GNN(pl.LightningModule):
    def __init__(
            self,
            n_node_classes: int,
            n_edge_classes: int,
            domain_size: int,
            out_channels: int,
            gnn_layer: str,
            depth: int,
            allow_floats_adj: bool = False,
            val_metrics_file: str = None,
    ):
        super().__init__()
        self.validation_step_outputs = []
        self.allow_floats_adj = allow_floats_adj
        self.val_metrics_file = val_metrics_file

        # 初始化最高指标记录
        self.highest_val_accuracy = 0.0
        self.highest_val_precision = 0.0
        self.highest_val_recall = 0.0
        self.best_epoch = -1
        
        # 新增：保存所有epoch的指标
        self.all_epoch_metrics = []

        self.input_layer = getattr(tg.nn, gnn_layer)(
            in_channels=n_node_classes,
            out_channels=out_channels,
        )
        # self.input_layer = torch.nn.Linear(n_node_classes, out_channels)
        self.hidden_layers = torch.nn.ModuleList(
            [
                getattr(tg.nn, gnn_layer)(
                    in_channels=out_channels,
                    out_channels=out_channels,
                )
                for _ in range(depth - 1)
            ]
        )
        self.batch_norms = torch.nn.ModuleList(
            [
                tg.nn.norm.BatchNorm(out_channels)
                for _ in range(depth)
            ]
        )
        # self.aggregator = torch.nn.Linear(out_channels, 1)
        self.aggregator = tg.nn.MeanAggregation()
        # self.classifier = torch.nn.Linear(domain_size * n_edge_classes, 1)
        self.classifier = torch.nn.Linear(out_channels, 1)

    def process_subgraph(self, node_features, adj_matrix):
        assert node_features.dim() == 3 and node_features.size(1) == adj_matrix.size(
            1
        ), f"{node_features.shape=}, {adj_matrix.shape=}, [batch_size, n_nodes, n_feat]"
        assert adj_matrix.dim() == 3 and adj_matrix.size(1) == adj_matrix.size(
            2
        ), f"{adj_matrix.shape=}, [batch_size, n_nodes, n_nodes]"
        assert self.allow_floats_adj or bool(
            ((adj_matrix == 0) | (adj_matrix == 1)).all()
        ), "Adj matrix should be zeros and ones"

        print(node_features.shape, adj_matrix.shape)
        x = self.input_layer(node_features, adj_matrix)
        # 乘回去方便数边数
        x = x * adj_matrix.sum(dim=-1, keepdim=True).clamp(min=1)
        # x = torch.nn.functional.tanh(x)
        x = torch.nn.functional.relu(x)
        x = self.batch_norms[0](x.permute(0, 2, 1)).permute(0, 2, 1)

        for idx, layer in enumerate(self.hidden_layers):
            x = layer(x, adj_matrix)
            # 乘回去方便数边数
            x = x * adj_matrix.sum(dim=-1, keepdim=True).clamp(min=1)
            x = torch.nn.functional.relu(x)
            x = self.batch_norms[idx + 1](x.permute(0, 2, 1)).permute(0, 2, 1)

        x = self.aggregator(x).squeeze(-1)
        # x = torch.nn.functional.tanh(x)

        return x

    def forward(self, node_features, edge_type_to_adj_matrix):
        subgraphs = [
            self.process_subgraph(node_features, adj_matrix)
            for adj_matrix in edge_type_to_adj_matrix.values()
        ]
        x = torch.cat(subgraphs, dim=-1)

        x = self.classifier(x).reshape(-1)
        x = torch.nn.functional.sigmoid(x)

        return x

    def training_step(self, batch, batch_idx):
        out = self(batch.node_feats, batch.edge_type_to_adj_matrix)
        loss = torch.nn.functional.binary_cross_entropy(out, batch.gan_y)
        # self.log("train_loss", loss, on_epoch=True)
        self.log("train_loss", loss, batch_size=len(batch.gan_y))
        self.log(
            "train_accuracy", accuracy(out, batch.gan_y), batch_size=len(batch.gan_y)
        )
        # self.log(
        #     "train_precision", precision(out, batch.gan_y), batch_size=len(batch.gan_y)
        # )
        # self.log("train_recall", recall(out, batch.gan_y), batch_size=len(batch.gan_y))
        return loss

    def validation_step(self, batch, batch_idx):
        out = self(batch.node_feats, batch.edge_type_to_adj_matrix)
        loss = torch.nn.functional.binary_cross_entropy(out, batch.gan_y)
        self.log("val_loss", loss, batch_size=len(batch.gan_y), prog_bar=True)

        val_accuracy = accuracy(out, batch.gan_y)
        val_precision = precision(out, batch.gan_y)
        val_recall = recall(out, batch.gan_y)

        self.log("val_accuracy", val_accuracy, batch_size=len(batch.gan_y), prog_bar=True)
        self.log("val_precision", val_precision, batch_size=len(batch.gan_y), prog_bar=True)
        self.log("val_recall", val_recall, batch_size=len(batch.gan_y), prog_bar=True)
        result = {
            "val_accuracy": val_accuracy,
            "val_precision": val_precision,
            "val_recall": val_recall,
            "batch_size": len(batch.gan_y)
        }
        self.validation_step_outputs.append(result)
        return result

    def on_validation_epoch_end(self) -> None:
        outputs = self.validation_step_outputs
        if not outputs:  # 空输出保护
            return

        def to_scalar(x):
            return x.item() if isinstance(x, torch.Tensor) else float(x)

        accuracies = [to_scalar(x["val_accuracy"]) for x in outputs]
        precisions = [to_scalar(x["val_precision"]) for x in outputs]
        recalls = [to_scalar(x["val_recall"]) for x in outputs]
        batch_sizes = [to_scalar(x["batch_size"]) for x in outputs]

        total_samples = sum(batch_sizes)
        if total_samples == 0:
            val_accuracy = val_precision = val_recall = 0.0
        else:
            val_accuracy = sum(acc * size for acc, size in zip(accuracies, batch_sizes)) / total_samples
            val_precision = sum(prec * size for prec, size in zip(precisions, batch_sizes)) / total_samples
            val_recall = sum(recall * size for recall, size in zip(recalls, batch_sizes)) / total_samples
        
        # 保存当前epoch的指标到列表
        epoch_metric = {
            "epoch": self.current_epoch,
            "val_accuracy": float(val_accuracy),
            "val_precision": float(val_precision),
            "val_recall": float(val_recall),
            "total_samples": int(total_samples)
        }
        self.all_epoch_metrics.append(epoch_metric)
        
        # 更新历史最佳指标
        if val_accuracy > self.highest_val_accuracy:
            self.highest_val_accuracy = val_accuracy
            self.highest_val_precision = val_precision
            self.highest_val_recall = val_recall
            self.best_epoch = self.current_epoch

        # 记录到日志
        self.log_dict(
            {
                "val/epoch_accuracy": val_accuracy,
                "val/epoch_precision": val_precision,
                "val/epoch_recall": val_recall,
                "val/best_accuracy": self.highest_val_accuracy,
                "val/best_precision": self.highest_val_precision,
                "val/best_recall": self.highest_val_recall,
                "val/best_epoch": self.best_epoch,
            },
            prog_bar=True,
            logger=True
        )

        # 清空批次输出列表
        self.validation_step_outputs.clear()
    def on_train_end(self):
        """训练结束时调用，保存最佳epoch的指标到文件"""
        if not self.all_epoch_metrics:
            print("警告：没有验证指标可保存")
            return
        
        # 找到准确率最高的epoch
        best_epoch_metric = max(self.all_epoch_metrics, key=lambda x: x["val_accuracy"])
        
        # 创建要保存的最佳结果
        best_result = {
            "run_id": int(time.time()),  # 唯一运行ID
            "training_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "best_epoch": best_epoch_metric["epoch"],
            "val_accuracy": best_epoch_metric["val_accuracy"],
            "val_precision": best_epoch_metric["val_precision"],
            "val_recall": best_epoch_metric["val_recall"],
            "total_samples": best_epoch_metric["total_samples"],
            "all_epochs_trained": self.current_epoch + 1,
            "total_epoch_metrics": len(self.all_epoch_metrics)
        }
        
        # 保存到文件
        if self.val_metrics_file:
            # 确保目录存在
            os.makedirs(os.path.dirname(self.val_metrics_file), exist_ok=True)
            
            all_results = []
            
            # 如果文件已存在，先读取已有的结果
            if os.path.exists(self.val_metrics_file) and os.path.getsize(self.val_metrics_file) > 0:
                try:
                    with open(self.val_metrics_file, "r", encoding="utf-8") as f:
                        all_results = json.load(f)
                        # 确保是列表格式
                        if not isinstance(all_results, list):
                            all_results = [all_results]  # 如果是单个对象，转换为列表
                except json.JSONDecodeError:
                    print("警告：无法解析现有文件，将创建新文件")
                    all_results = []
            
            # 添加新的结果
            all_results.append(best_result)
            
            # 保存所有结果
            with open(self.val_metrics_file, "w", encoding="utf-8") as f:
                json.dump(all_results, f, indent=2, ensure_ascii=False)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)
        return optimizer


def accuracy(pred: torch.Tensor, target: torch.Tensor) -> float:
    pred_hard = (pred > 0.5).float()
    return (pred_hard == target).float().mean().item()


def precision(pred: torch.Tensor, target: torch.Tensor) -> float:
    pred_hard = (pred > 0.5).float()
    true_positives = ((pred_hard == 1) & (target == 1)).float().sum().item()
    predicted_positives = (pred_hard == 1).float().sum().item()
    return true_positives / predicted_positives if predicted_positives > 0 else 0.0


def recall(pred: torch.Tensor, target: torch.Tensor) -> float:
    pred_hard = (pred > 0.5).float()
    true_positives = ((pred_hard == 1) & (target == 1)).float().sum().item()
    actual_positives = (target == 1).float().sum().item()
    return true_positives / actual_positives if actual_positives > 0 else 0.0


if __name__ == "__main__":
    app()
